{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Level 2 - Beautiful Soup\n",
    "\n",
    "---\n",
    "\n",
    "# The Mission\n",
    "\n",
    "Your company `SpiderLegion` has just signed a contract with an Analytics Company called `DashItUp`.\n",
    "`DashItUp` is well known for it's dashboarding capabilities specializing in monitoring website metrics such as views, content shares, new users, and database errors!\n",
    "\n",
    "While dashboards are nice, `DashItUp` is now wanting to spend some time on a new `summarize` feature. \n",
    "`DashItUp` wants to run web crawlers against their dashboards to fetch the `key metrics` and print them off as a single report.\n",
    "\n",
    "## Key Metrics\n",
    "\n",
    "* User Count\n",
    "* Any _system errors_, how recent?\n",
    "    * System errors can be one of the following: `Database error`, `CPU overload`, or `Out of memory`\n",
    "* Bounce Rate\n",
    "* Top and bottom countries by utility\n",
    "* Most recent user names with links to their profiles\n",
    "* Name of the user that owns the dashboard\n",
    "\n",
    "`DashItUp` has _many_ websites that use the same template (they all look the same). \n",
    "They believe that if you can write a web crawler for one, they should be able to apply the same code to the other dashboards they own to get similar results.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch The Website Contents\n",
    "\n",
    "`DashItUp` was kind enough to give us a website to test against.\n",
    "The website content can be found in the `assets` folder called `website.html`.\n",
    "We already have some code that is responsible for opening that file, reading it, and saving the contents to a variable called `website_contents`.\n",
    "\n",
    "(Source HTML code is from the Analytics Template from the website https://www.w3schools.com/w3css/w3css_templates.asp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(\"../assets/website.html\") as website_file:\n",
    "    website_contents = website_file.read()\n",
    "    \n",
    "website_contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What a jumbled mess!\n",
    "It is nearly impossible to understand what is going on here without some hardcore `HTML` understanding..\n",
    "Unless we visualize it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# In jupyter, you can visualize raw HTML using these two functions!\n",
    "# It is essentially \"embedding\" the website content within the notebook\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(HTML(website_contents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Get to Work!\n",
    "\n",
    "### Import the tools needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Create the Soup!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here\n",
    "\n",
    "soup = BeautifulSoup(website_contents, \"html.parser\")\n",
    "soup.title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## User Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here\n",
    "\n",
    "# The answer we are looking for lives within the \n",
    "# \"da-dashboardCards\" section of the website, so target that first\n",
    "dashboard_cards_soup = soup.find(\"div\", attrs={\"class\": \"da-dashboardCards\"})\n",
    "\n",
    "dashboard_cards_soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# grab all of the direct children\n",
    "dashboard_cards = dashboard_cards_soup.findAll(recursive=False)\n",
    "\n",
    "dashboard_cards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Users is the LAST child.. Since it's a list, we can target that!\n",
    "user_card = dashboard_cards[-1]\n",
    "\n",
    "user_card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_card.find(\"h3\", attrs={\"class\": \"da-dashboardCardMetric\"}).text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Any _system errors_, how recent?\n",
    "System errors can be one of the following: \n",
    "\n",
    "* `Database error`\n",
    "* `CPU overload`\n",
    "* `Out of memory`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# code here\n",
    "\n",
    "# The content we want lives within the \"da-feeds\" section of the website,\n",
    "# so we can target that first!\n",
    "feeds = soup.find(\"div\", attrs={\"class\": \"da-feeds\"})\n",
    "\n",
    "feeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since the result lives in a table, we can use pandas to extract it\n",
    "# and put it in a dataframe for us!\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "dataframes = pd.read_html(str(feeds))\n",
    "dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feed_dataframe = dataframes[0]\n",
    "feed_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It may be easier if we clean up this dataframe a little first\n",
    "feed_dataframe.columns = [\"icon\", \"message\", \"minutes\"]\n",
    "feed_dataframe = feed_dataframe.drop(columns=[\"icon\"])\n",
    "\n",
    "feed_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now, all we need to do is filter it down!\n",
    "error_messages = [\n",
    "    \"Database error.\",\n",
    "    \"CPU overload.\",\n",
    "    \"Out of memory.\"\n",
    "]\n",
    "\n",
    "error_dataframe = feed_dataframe[feed_dataframe[\"message\"].isin(error_messages)]\n",
    "error_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Bounce Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here\n",
    "\n",
    "# This one is a bit simpler since the element has an \"id\" attribute\n",
    "# We can use that (\"da-bounceRateStat\") to target the element directly\n",
    "soup.find(id=\"da-bounceRateStat\").text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to get rid of the newlines..\n",
    "soup.find(id=\"da-bounceRateStat\").text \\\n",
    "    .replace(\"\\n\", \"\") \\\n",
    "    .replace(\"%\", \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Top and bottom countries by utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# code here\n",
    "\n",
    "# This one will be similar to the feeds! Pandas to the rescue.\n",
    "# First, let's target the HTML that includes our table\n",
    "country_utility_soup = soup.find(attrs={\"class\": \"da-countryUtility\"})\n",
    "country_utility_soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_utility_tables = pd.read_html(str(country_utility_soup))\n",
    "country_utility_table = country_utility_tables[0]\n",
    "country_utility_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we want to grab the top and bottom country by utility\n",
    "# Let's see what the datatypes are!\n",
    "country_utility_table.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ok, they are \"object\"/\"text\" types..\n",
    "# We want to handle that before sorting.\n",
    "country_utility_table[\"Utility\"] = country_utility_table[\"Utility\"] \\\n",
    "    .str.replace(\"%\", \"\") \\\n",
    "    .astype(\"float64\")\n",
    "\n",
    "country_utility_table.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we can sort and reset the index\n",
    "country_utility_table = country_utility_table \\\n",
    "    .sort_values(by=\"Utility\", ascending=False) \\\n",
    "    .reset_index(drop=True)\n",
    "\n",
    "country_utility_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab the top..\n",
    "country_utility_table.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab the bottom\n",
    "country_utility_table.tail(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Most recent user names with links to their profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# code here\n",
    "\n",
    "# This one is a little trickier since it is not structured as a table, \n",
    "# and we are also trying to grab 2 things!\n",
    "# This sounds like a job for the trusty for-loop\n",
    "\n",
    "# Start by narrowing down the HTML to the area of interest, \"da-recentUsers\"\n",
    "recent_users = soup.find(attrs={\"class\": \"da-recentUsers\"})\n",
    "\n",
    "print(recent_users.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Luckily, we can see a pattern here! \n",
    "# Let's assess the structure\n",
    "\"\"\"\n",
    "\n",
    "<div class=\"... da-recentUsers\">\n",
    "  ...\n",
    "  <ul ...>  --------------------------------- start of the loop\n",
    "    <li>  ----------------------------------- element within loop\n",
    "      <a href=\"PROFILE URL HERE!!\"> --------- element url!!\n",
    "        ...\n",
    "        <span ...>PROFILE NAME HERE!</span> - element name!!\n",
    "      </a>\n",
    "    </li>\n",
    "    <li>...</li> ---------------------------- next element\n",
    "    <li>...</li> ---------------------------- next element\n",
    "  </ul>\n",
    "</div>\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# SO!! The <ul> element makes up the \"base\" of our structure.\n",
    "# Every child element within (<li>) represents a different recent user.\n",
    "# Therefore, if we loop over the elements of the <ul> element,  \n",
    "# we should be able to extract the URL and name per user.\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's start by just trying to loop over the <li> elements, \n",
    "# then we can build off of that.\n",
    "for list_element in recent_users.find(\"ul\").findAll(\"li\"):\n",
    "    print(\"\\n\", str(list_element.prettify()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ok, now let's try to grab the href from the <a> tag\n",
    "for list_element in recent_users.find(\"ul\").findAll(\"li\"):\n",
    "    a_tag = list_element.find(\"a\")\n",
    "    print(a_tag[\"href\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's try to grab the user name from the <span> tag\n",
    "for list_element in recent_users.find(\"ul\").findAll(\"li\"):\n",
    "    span_tag = list_element.find(\"span\")\n",
    "    print(span_tag.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, put them together\n",
    "# Let's try to grab the user name from the <span> tag\n",
    "for list_element in recent_users.find(\"ul\").findAll(\"li\"):\n",
    "    a_tag = list_element.find(\"a\")\n",
    "    span_tag = list_element.find(\"span\")\n",
    "    \n",
    "    print(span_tag.text, a_tag[\"href\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And finally, add them to a list to be used later!\n",
    "recent_user_info = []\n",
    "\n",
    "for list_element in recent_users.find(\"ul\").findAll(\"li\"):\n",
    "    a_tag = list_element.find(\"a\")\n",
    "    span_tag = list_element.find(\"span\")\n",
    "    \n",
    "    recent_user_info.append([span_tag.text, a_tag[\"href\"]])\n",
    "    \n",
    "recent_user_info  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Name of the user that owns the dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here\n",
    "\n",
    "# This is a trick question! Try opening the \"website.html\" in your browser\n",
    "# and see the \"responsiveness\" of the website.\n",
    "# When the website gets below a certain width, the \"menu\" gets hidden!\n",
    "# We can't see it in the notebook..\n",
    "# But that doesn't mean it is not there.\n",
    "\n",
    "# The info that we need lives within the \"da-welcomeMenu\" element\n",
    "welcome_menu = soup.find(attrs={\"class\": \"da-welcomeMenu\"})\n",
    "welcome_menu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "welcome_menu.find(\"strong\").text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
